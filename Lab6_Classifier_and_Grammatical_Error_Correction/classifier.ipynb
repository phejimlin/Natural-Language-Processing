{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gen_feature_set(tags, offset):\n",
    "    def tags_to_word_pos(tags):\n",
    "        if tags[0] is None:\n",
    "            return ''\n",
    "        else :\n",
    "            return \" \".join((tag[0] + \"_\" + tag[2] for tag in tags))\n",
    "    \n",
    "    def find_FH(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # Lemma of headword of following phrase with PoS tag\n",
    "        temp_tag = None\n",
    "        for tag in tags[offset+1:sentence_length]:\n",
    "            if tag[3][0]=='B' and temp_tag is None:\n",
    "                temp_tag = tag\n",
    "            if temp_tag is not None and tag[3][0]=='I':\n",
    "                return tag\n",
    "            \n",
    "        if temp_tag is None:\n",
    "            return ('','','','')\n",
    "        return temp_tag\n",
    "    \n",
    "    def find_FP(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # Following phrase including PoS tags\n",
    "        temp_tag = None\n",
    "        for tag in tags[offset+1:sentence_length]:\n",
    "            if tag[3][0]=='B' and temp_tag is None:\n",
    "                temp_tag = [tag]\n",
    "            if temp_tag is not None and tag[3][0]=='I':\n",
    "                temp_tag.append(tag)\n",
    "                return temp_tag\n",
    "\n",
    "        if temp_tag is None:\n",
    "            return [None]\n",
    "        return temp_tag\n",
    "    \n",
    "    def find_PHR_pre(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # Preceding phrase type\n",
    "        temp_tag = ''\n",
    "        for tag in tags[0:offset]:\n",
    "            if tag[3][0]=='B':\n",
    "                temp_tag = tag[3].split('-')[1]\n",
    "        return temp_tag \n",
    "\n",
    "    def find_PV(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # Preceding verb lemma with PoS tag\n",
    "        temp_tag = None\n",
    "        for tag in tags[0:offset]:\n",
    "            if tag[2][0]=='V':\n",
    "                temp_tag = tag\n",
    "        return temp_tag\n",
    "\n",
    "        \n",
    "    def find_FHtag(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # PoS tag of headword of the following phrase\n",
    "        temp_tag = None\n",
    "        for tag in tags[offset+1:sentence_length]:\n",
    "            if tag[3][0]=='B' and temp_tag is None:\n",
    "                temp_tag = tag\n",
    "            if temp_tag is not None and tag[3][0]=='I':\n",
    "                return tag\n",
    "\n",
    "        if temp_tag is None:\n",
    "            return ('','','','')\n",
    "        return temp_tag\n",
    "    \n",
    "    def find_PVtag(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # PoS tag of the preceding verb\n",
    "        temp_tag = None\n",
    "        for tag in tags[0:offset]:\n",
    "            if tag[2][0]=='V':\n",
    "                temp_tag = tag\n",
    "\n",
    "        if temp_tag is None:\n",
    "            return ('','','','')\n",
    "        return temp_tag\n",
    "    \n",
    "    features = {}\n",
    "    features['TGLR'] = tags_to_word_pos([tags[offset - 1], tags[offset + 1]]) if 0 < offset < len(tags) - 1 else ''\n",
    "    features['TGL'] = tags_to_word_pos([tags[offset - 2], tags[offset - 1]]) if 1 < offset else ''\n",
    "    features['TGR'] = tags_to_word_pos([tags[offset + 1], tags[offset + 2]]) if offset < len(tags) - 2 else ''\n",
    "    features['BGL'] = tags_to_word_pos([tags[offset - 1]]) if 0 < offset else ''\n",
    "    features['BGR'] = tags_to_word_pos([tags[offset + 1]]) if offset < len(tags) - 1 else ''\n",
    "    features['FH'] = tags_to_word_pos([find_FH(tags, offset)]) \n",
    "    features['FP'] = tags_to_word_pos(find_FP(tags, offset))\n",
    "    features['FHword'] = find_FH(tags, offset)[0]\n",
    "    features['PHR_pre'] = find_PHR_pre(tags, offset)\n",
    "    features['PV'] = tags_to_word_pos([find_PV(tags, offset)]) if 0 < offset else ''\n",
    "    features['FHtag'] = find_FHtag(tags, offset)[2]\n",
    "    features['PVtag'] = find_PVtag(tags, offset)[2] if 0 < offset else ''\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BGL': 'last_JJ',\n",
       " 'BGR': 'a_DT',\n",
       " 'FH': 'boot_NN',\n",
       " 'FHtag': 'NN',\n",
       " 'FHword': 'boot',\n",
       " 'FP': 'a_DT boot_NN',\n",
       " 'PHR_pre': 'ADJP',\n",
       " 'PV': 'use_VB',\n",
       " 'PVtag': 'VB',\n",
       " 'TGL': 'style_NN last_JJ',\n",
       " 'TGLR': 'last_JJ a_DT',\n",
       " 'TGR': 'a_DT boot_NN'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_feature_set([('For', 'For', 'IN', 'B-PP'), ('example', 'example', 'NN', 'B-NP'), (',', ',', ',', 'O'), ('an', 'an', 'DT', 'B-NP'), ('open-toe', 'open-toe', 'JJ', 'I-NP'), ('sandal', 'sandal', 'NN', 'I-NP'), ('will', 'will', 'MD', 'B-VP'), ('use', 'use', 'VB', 'I-VP'), ('a', 'a', 'DT', 'B-NP'), ('different', 'different', 'JJ', 'I-NP'), ('style', 'style', 'NN', 'I-NP'), ('last', 'last', 'JJ', 'B-ADJP'), ('to', 'to', 'TO', 'B-PP'), ('a', 'a', 'DT', 'B-NP'), ('boot', 'boot', 'NN', 'I-NP'), ('.', '.', '.', 'O')], 12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "def split_mul(text):\n",
    "    data_list = text.split(\"\\t\")\n",
    "    return list(zip(data_list[0].split(' '), data_list[1].split(' '), data_list[2].split(' '), data_list[3].split(' ')))\n",
    "\n",
    "def split_t(text):\n",
    "    return text.split(\"\\t\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    corrections = list(map(split_t, open('./data10k/wiki.prep.corrections.clean.10k')))\n",
    "    data = list(map(split_mul, open(\"./data10k/wiki.prep.sents.clean.genia.10k\")))\n",
    "    \n",
    "    size = len(data)\n",
    "    train = data[:-(size//10)]\n",
    "    test = data[-(size//10):]\n",
    "\n",
    "    # prepare your train and test data\n",
    "    featuresets = []\n",
    "    for index, tags in enumerate(data):\n",
    "        offset = int(corrections[index][0]) - 1\n",
    "        featuresets.append((gen_feature_set(tags, offset), corrections[index][3]))\n",
    "    \n",
    "    split_point = len(featuresets)*9//10\n",
    "    trainData = featuresets[:split_point]\n",
    "    testData = featuresets[split_point:]\n",
    "\n",
    "    # train your classifier\n",
    "#     classifier = SklearnClassifier(LogisticRegression())\n",
    "#     classifier.train(trainData)\n",
    "\n",
    "    # test your classifier\n",
    "#     # ...\n",
    "#     correct = 0\n",
    "\n",
    "#     precision = correct / len(testData)\n",
    "\n",
    "#     print('precision:', precision)\n",
    "#     print('recall:')\n",
    "#     print('f1-measure:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'BGL': 'last_JJ',\n",
       "   'BGR': 'a_DT',\n",
       "   'FH': 'boot_NN',\n",
       "   'FHtag': 'NN',\n",
       "   'FHword': 'boot',\n",
       "   'FP': 'a_DT boot_NN',\n",
       "   'PHR_pre': 'ADJP',\n",
       "   'PV': 'use_VB',\n",
       "   'PVtag': 'VB',\n",
       "   'TGL': 'style_NN last_JJ',\n",
       "   'TGLR': 'last_JJ a_DT',\n",
       "   'TGR': 'a_DT boot_NN'},\n",
       "  'from')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 149 ms, total: 11.2 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.classify import SklearnClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%time sklearn_classifier = SklearnClassifier(LogisticRegression()).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -4.81218        0.001\n",
      "             2          -0.62498        0.942\n",
      "             3          -0.24841        0.976\n",
      "             4          -0.15927        0.985\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.12019        0.989\n",
      "CPU times: user 5min 56s, sys: 2.11 s, total: 5min 58s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "%time nltk_classifier = MaxentClassifier.train(trainData, nltk.classify.MaxentClassifier.ALGORITHMS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== SkLearn MaxEnt ==\n",
      "from\n",
      "0.393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('== SkLearn MaxEnt ==')\n",
    "print(sklearn_classifier.classify(gen_feature_set([('For', 'For', 'IN', 'B-PP'), ('example', 'example', 'NN', 'B-NP'), (',', ',', ',', 'O'), ('an', 'an', 'DT', 'B-NP'), ('open-toe', 'open-toe', 'JJ', 'I-NP'), ('sandal', 'sandal', 'NN', 'I-NP'), ('will', 'will', 'MD', 'B-VP'), ('use', 'use', 'VB', 'I-VP'), ('a', 'a', 'DT', 'B-NP'), ('different', 'different', 'JJ', 'I-NP'), ('style', 'style', 'NN', 'I-NP'), ('last', 'last', 'JJ', 'B-ADJP'), ('to', 'to', 'TO', 'B-PP'), ('a', 'a', 'DT', 'B-NP'), ('boot', 'boot', 'NN', 'I-NP'), ('.', '.', '.', 'O')], 12)))\n",
    "print(nltk.classify.accuracy(sklearn_classifier, testData))\n",
    "print()\n",
    "# print('== NLTK MaxEnt ==')\n",
    "# print(nltk_classifier.classify(gender_features('mark')))\n",
    "# print(nltk.classify.accuracy(nltk_classifier, test_set))\n",
    "# print(nltk_classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
