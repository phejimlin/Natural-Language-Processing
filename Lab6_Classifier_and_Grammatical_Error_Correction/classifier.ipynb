{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gen_feature_set(tags, offset):\n",
    "    def tags_to_word_pos(tags):\n",
    "        if tags[0] is None:\n",
    "            return ''\n",
    "        else :\n",
    "            return \" \".join((tag[0] + \"_\" + tag[2] for tag in tags))\n",
    "    \n",
    "    def find_FH(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # Lemma of headword of following phrase with PoS tag\n",
    "        temp_tag = None\n",
    "        for tag in tags[offset+1:sentence_length]:\n",
    "            if tag[3][0]=='B' and temp_tag is None:\n",
    "                temp_tag = tag\n",
    "            if temp_tag is not None and tag[3][0]=='I':\n",
    "                return tag\n",
    "            \n",
    "        if temp_tag is None:\n",
    "            return ('','','','')\n",
    "        return temp_tag\n",
    "    \n",
    "    def find_FP(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # Following phrase including PoS tags\n",
    "        temp_tag = None\n",
    "        for tag in tags[offset+1:sentence_length]:\n",
    "            if tag[3][0]=='B' and temp_tag is None:\n",
    "                temp_tag = [tag]\n",
    "            if temp_tag is not None and tag[3][0]=='I':\n",
    "                temp_tag.append(tag)\n",
    "                return temp_tag\n",
    "\n",
    "        if temp_tag is None:\n",
    "            return [None]\n",
    "        return temp_tag\n",
    "    \n",
    "    def find_PHR_pre(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # Preceding phrase type\n",
    "        temp_tag = ''\n",
    "        for tag in tags[0:offset]:\n",
    "            if tag[3][0]=='B':\n",
    "                temp_tag = tag[3].split('-')[1]\n",
    "        return temp_tag \n",
    "\n",
    "    def find_PV(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # Preceding verb lemma with PoS tag\n",
    "        temp_tag = None\n",
    "        for tag in tags[0:offset]:\n",
    "            if tag[2][0]=='V':\n",
    "                temp_tag = tag\n",
    "        return temp_tag\n",
    "\n",
    "        \n",
    "    def find_FHtag(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # PoS tag of headword of the following phrase\n",
    "        temp_tag = None\n",
    "        for tag in tags[offset+1:sentence_length]:\n",
    "            if tag[3][0]=='B' and temp_tag is None:\n",
    "                temp_tag = tag\n",
    "            if temp_tag is not None and tag[3][0]=='I':\n",
    "                return tag\n",
    "\n",
    "        if temp_tag is None:\n",
    "            return ('','','','')\n",
    "        return temp_tag\n",
    "    \n",
    "    def find_PVtag(tags, offset):\n",
    "        sentence_length = len(tags)\n",
    "        # PoS tag of the preceding verb\n",
    "        temp_tag = None\n",
    "        for tag in tags[0:offset]:\n",
    "            if tag[2][0]=='V':\n",
    "                temp_tag = tag\n",
    "\n",
    "        if temp_tag is None:\n",
    "            return ('','','','')\n",
    "        return temp_tag\n",
    "    \n",
    "    features = {}\n",
    "    features['TGLR'] = tags_to_word_pos([tags[offset - 1], tags[offset + 1]]) if 0 < offset < len(tags) - 1 else ''\n",
    "    features['TGL'] = tags_to_word_pos([tags[offset - 2], tags[offset - 1]]) if 1 < offset else ''\n",
    "    features['TGR'] = tags_to_word_pos([tags[offset + 1], tags[offset + 2]]) if offset < len(tags) - 2 else ''\n",
    "    features['BGL'] = tags_to_word_pos([tags[offset - 1]]) if 0 < offset else ''\n",
    "    features['BGR'] = tags_to_word_pos([tags[offset + 1]]) if offset < len(tags) - 1 else ''\n",
    "    features['FH'] = tags_to_word_pos([find_FH(tags, offset)]) \n",
    "    features['FP'] = tags_to_word_pos(find_FP(tags, offset))\n",
    "    features['FHword'] = find_FH(tags, offset)[0]\n",
    "    features['PHR_pre'] = find_PHR_pre(tags, offset)\n",
    "    features['PV'] = tags_to_word_pos([find_PV(tags, offset)]) if 0 < offset else ''\n",
    "    features['FHtag'] = find_FHtag(tags, offset)[2]\n",
    "    features['PVtag'] = find_PVtag(tags, offset)[2] if 0 < offset else ''\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BGL': 'last_JJ',\n",
       " 'BGR': 'a_DT',\n",
       " 'FH': 'boot_NN',\n",
       " 'FHtag': 'NN',\n",
       " 'FHword': 'boot',\n",
       " 'FP': 'a_DT boot_NN',\n",
       " 'PHR_pre': 'ADJP',\n",
       " 'PV': 'use_VB',\n",
       " 'PVtag': 'VB',\n",
       " 'TGL': 'style_NN last_JJ',\n",
       " 'TGLR': 'last_JJ a_DT',\n",
       " 'TGR': 'a_DT boot_NN'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feture_test = [('For', 'For', 'IN', 'B-PP'), ('example', 'example', 'NN', 'B-NP'), (',', ',', ',', 'O'), ('an', 'an', 'DT', 'B-NP'), ('open-toe', 'open-toe', 'JJ', 'I-NP'), ('sandal', 'sandal', 'NN', 'I-NP'), ('will', 'will', 'MD', 'B-VP'), ('use', 'use', 'VB', 'I-VP'), ('a', 'a', 'DT', 'B-NP'), ('different', 'different', 'JJ', 'I-NP'), ('style', 'style', 'NN', 'I-NP'), ('last', 'last', 'JJ', 'B-ADJP'), ('to', 'to', 'TO', 'B-PP'), ('a', 'a', 'DT', 'B-NP'), ('boot', 'boot', 'NN', 'I-NP'), ('.', '.', '.', 'O')]\n",
    "gen_feature_set(feture_test, 12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import re\n",
    "\n",
    "def split_mul(text):\n",
    "    data_list = text.split(\"\\t\")\n",
    "    return list(zip(data_list[0].split(' '), data_list[1].split(' '), data_list[2].split(' '), data_list[3].split(' ')))\n",
    "\n",
    "def split_t(text):\n",
    "    return text.split(\"\\t\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    corrections = list(map(split_t, open('./data100k/wiki.prep.corrections.clean.100k')))\n",
    "    data = list(map(split_mul, open(\"./data100k/wiki.prep.sents.clean.genia.100k\")))\n",
    "    \n",
    "    size = len(data)\n",
    "    train = data[:-(size//10)]\n",
    "    test = data[-(size//10):]\n",
    "\n",
    "    # prepare your train and test data\n",
    "    featuresets = []\n",
    "    for index, tags in enumerate(data):\n",
    "        offset = int(corrections[index][0]) - 1\n",
    "        \n",
    "        answer = re.sub('[^0-9a-zA-Z]+', '', corrections[index][3].lower())\n",
    "        original_answer = re.sub('[^0-9a-zA-Z]+', '', corrections[index][1].lower())\n",
    "        featuresets.append(((gen_feature_set(tags, offset), answer), original_answer))\n",
    "    \n",
    "    split_point = len(featuresets)*9//10\n",
    "    train_data_total = featuresets[:split_point]\n",
    "    test_data_total = featuresets[split_point:]\n",
    "    \n",
    "    trainData = list(map(lambda a: a[0], train_data_total))\n",
    "    testData = list(map(lambda a: a[0], test_data_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'BGL': 'last_JJ',\n",
       "   'BGR': 'a_DT',\n",
       "   'FH': 'boot_NN',\n",
       "   'FHtag': 'NN',\n",
       "   'FHword': 'boot',\n",
       "   'FP': 'a_DT boot_NN',\n",
       "   'PHR_pre': 'ADJP',\n",
       "   'PV': 'use_VB',\n",
       "   'PVtag': 'VB',\n",
       "   'TGL': 'style_NN last_JJ',\n",
       "   'TGLR': 'last_JJ a_DT',\n",
       "   'TGR': 'a_DT boot_NN'},\n",
       "  'from')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 13s, sys: 52min 31s, total: 1h 9min 45s\n",
      "Wall time: 4min 26s\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.classify import SklearnClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%time sklearn_classifier = SklearnClassifier(LogisticRegression()).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== SkLearn MaxEnt ==\n",
      "from\n",
      "0.4626\n"
     ]
    }
   ],
   "source": [
    "print('== SkLearn MaxEnt ==')\n",
    "print(sklearn_classifier.classify(gen_feature_set(feture_test, 12)))\n",
    "print(nltk.classify.accuracy(sklearn_classifier, testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'BGL': 'capacity_NN',\n",
       "   'BGR': 'power_NN',\n",
       "   'FH': 'generation_NN',\n",
       "   'FHtag': 'NN',\n",
       "   'FHword': 'generation',\n",
       "   'FP': 'power_NN generation_NN',\n",
       "   'PHR_pre': 'NP',\n",
       "   'PV': 'installed_VBN',\n",
       "   'PVtag': 'VBN',\n",
       "   'TGL': 'installed_VBN capacity_NN',\n",
       "   'TGLR': 'capacity_NN power_NN',\n",
       "   'TGR': 'power_NN generation_NN'},\n",
       "  'for')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behind: 0.000561\n",
      "afte: 0.000186\n",
      "over: 0.004591\n",
      "underneath: 0.000322\n",
      "up: 0.000651\n",
      "aboard: 0.000371\n",
      "alpoog: 0.000186\n",
      "abou: 0.000272\n",
      "around: 0.000824\n",
      "aebove: 0.000359\n",
      "after: 0.006642\n",
      "amongst: 0.000542\n",
      "during: 0.010799\n",
      "per: 0.000359\n",
      "asa: 0.000167\n",
      "onto: 0.043416\n",
      "that: 0.008059\n",
      "across: 0.001712\n",
      "below: 0.000235\n",
      "fhom: 0.000170\n",
      "beyond: 0.000717\n",
      "astride: 0.000428\n",
      "like: 0.002556\n",
      "til: 0.000230\n",
      "na: 0.000182\n",
      "nearer: 0.000183\n",
      "though: 0.000423\n",
      "arond: 0.000186\n",
      "unlike: 0.000241\n",
      "byetmn: 0.000236\n",
      "amidst: 0.000226\n",
      "albeit: 0.000464\n",
      "beneath: 0.000460\n",
      "accross: 0.000233\n",
      "with: 0.023962\n",
      "bewteen: 0.000174\n",
      "although: 0.001244\n",
      "alongside: 0.000620\n",
      "than: 0.021757\n",
      "versus: 0.000183\n",
      "on: 0.016411\n",
      "past: 0.000947\n",
      "notwithstanding: 0.000225\n",
      "aboards: 0.000170\n",
      "huggimg: 0.000254\n",
      "under: 0.001743\n",
      "en: 0.000179\n",
      "whilst: 0.000254\n",
      "ago: 0.000239\n",
      "wiff: 0.000192\n",
      "att: 0.000171\n",
      "since: 0.002180\n",
      "o: 0.000236\n",
      "along: 0.001795\n",
      "befor: 0.000164\n",
      "betwen: 0.000176\n",
      "except: 0.000335\n",
      "whether: 0.004016\n",
      "before: 0.001428\n",
      "within: 0.009024\n",
      "off: 0.002282\n",
      "ta: 0.000193\n",
      "against: 0.007157\n",
      "without: 0.001856\n",
      "outside: 0.000429\n",
      "at: 0.016186\n",
      "through: 0.006428\n",
      "for: 0.038493\n",
      "despite: 0.000719\n",
      "if: 0.008818\n",
      "adfter: 0.000187\n",
      "t: 0.000158\n",
      "de: 0.000236\n",
      "near: 0.000437\n",
      "above: 0.000436\n",
      "once: 0.000610\n",
      "in: 0.098776\n",
      "as: 0.079026\n",
      "tovbc: 0.000233\n",
      "besides: 0.000119\n",
      "towards: 0.003227\n",
      "between: 0.000279\n",
      "till: 0.000505\n",
      "because: 0.004228\n",
      "out: 0.000210\n",
      "toward: 0.002759\n",
      "unill: 0.000222\n",
      "about: 0.006388\n",
      "of: 0.010276\n",
      "thru: 0.000187\n",
      "throughout: 0.001972\n",
      "null: 0.070235\n",
      "down: 0.000602\n",
      "oer: 0.000167\n",
      "into: 0.012429\n",
      "to: 0.014093\n",
      "from: 0.394287\n",
      "opposite: 0.000127\n",
      "amid: 0.000442\n",
      "aet: 0.000175\n",
      "unless: 0.000725\n",
      "among: 0.000793\n",
      "altho: 0.000240\n",
      "by: 0.012279\n",
      "throughtout: 0.000180\n",
      "so: 0.000239\n",
      "beside: 0.000231\n",
      "inside: 0.004466\n",
      "whereas: 0.000411\n",
      "untill: 0.000343\n",
      "ass: 0.000170\n",
      "upon: 0.005437\n",
      "alnong: 0.000280\n",
      "via: 0.005047\n",
      "until: 0.005694\n",
      "fo: 0.000179\n",
      "athgfj: 0.000242\n",
      "atop: 0.000571\n",
      "while: 0.001610\n"
     ]
    }
   ],
   "source": [
    "pdist = sklearn_classifier.prob_classify(gen_feature_set(feture_test, 12))\n",
    "for label in pdist.samples():\n",
    "    print(\"%s: %f\" % (label, pdist.prob(label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of all data: 10000\n",
      "Correction: 4626\n",
      "Amount of data classifier altered: 9996\n",
      "precision: 0.46278511404561823\n",
      "recall: 0.4626\n",
      "f1-measure: 0.46269253850770153\n"
     ]
    }
   ],
   "source": [
    "# test your classifier without threshold\n",
    "correct = 0\n",
    "total_altered_data = len(testData)\n",
    "\n",
    "for data in test_data_total:\n",
    "    test_data = data[0]\n",
    "    original_label = data[1]\n",
    "\n",
    "    pdist = sklearn_classifier.prob_classify(test_data[0])\n",
    "    label = pdist.max()\n",
    "    label_prob = pdist.prob(label)\n",
    "    if test_data[1] == label:\n",
    "        correct += 1\n",
    "    elif test_data[1] == original_label:\n",
    "        total_altered_data-=1\n",
    "\n",
    "precision = correct / total_altered_data\n",
    "recall = correct / len(testData)\n",
    "\n",
    "print('Amount of all data:', len(testData))\n",
    "print('Correction:', correct)\n",
    "print('Amount of data classifier altered:' ,total_altered_data)\n",
    "\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('f1-measure:', 2*precision*recall / (precision+recall) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of all data: 10000\n",
      "Correction: 4573\n",
      "Amount of data classifier altered: 9550\n",
      "precision: 0.478848167539267\n",
      "recall: 0.4573\n",
      "f1-measure: 0.4678260869565217\n"
     ]
    }
   ],
   "source": [
    "# recall 要改的當中 我改對的\n",
    "# precision 我改得當中 我改對的\n",
    "\n",
    "# test your classifier\n",
    "correct = 0\n",
    "threshold = 0.1835\n",
    "total_altered_data = len(testData)\n",
    "\n",
    "for data in test_data_total:\n",
    "    test_data = data[0]\n",
    "    original_label = data[1]\n",
    "\n",
    "    pdist = sklearn_classifier.prob_classify(test_data[0])\n",
    "    label = pdist.max()\n",
    "    label_prob = pdist.prob(label)\n",
    "    if label_prob > threshold:\n",
    "        if test_data[1] == label:\n",
    "            correct += 1\n",
    "        elif test_data[1] == original_label:\n",
    "            total_altered_data-=1\n",
    "    else:\n",
    "        total_altered_data-=1\n",
    "\n",
    "precision = correct / total_altered_data\n",
    "recall = correct / len(testData)\n",
    "\n",
    "print('Amount of all data:', len(testData))\n",
    "print('Correction:', correct)\n",
    "print('Amount of data classifier altered:' ,total_altered_data)\n",
    "\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('f1-measure:', 2*precision*recall / (precision+recall) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
